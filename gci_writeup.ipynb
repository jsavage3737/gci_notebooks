{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Network Based Approach to Medical Image Recognition\n",
    "### Jack Savage, Kyle Burack, Tyler Seppala, Josh Michelberg\n",
    "\n",
    "---\n",
    "**Introduction**:\n",
    "\n",
    "Many communities around the world have little to no access to modern health care. Within our own country, there are two major groups with limited access: those without money for health-care, and those who are far from their nearest healthcare provider. In both circumstances listed, artificial intelligence provides a solution. In poverty-stricken areas, human doctors are costly while an open-source software-based diagnosis system would be cheap. In rural areas without access to experienced or well-trained doctors, medical technicians would be able to perform at a similar level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](doctor_shortages.jpg \"Doctor Shortages\")\n",
    "**Relevent graphic showcasing shortages of doctors within the United States**\n",
    "\n",
    "_Further information on the 'HPSA score' metric used in the graph can be found [here](https://bhw.hrsa.gov/shortage-designation/hpsas)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Talk about previous approaches to medical image recognition (CITE STUFF HERE) (JOSH)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Talk about how versatile neural networks are due to transfer learning and training process as a whole (CITE STUFF HERE) (JACK)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Past Work**:\n",
    "\n",
    "We began this project last semester with no knowledge of neural networks. ~75% of our work last semester was in researching our topic while ~25% was becoming familiar with the high-level API [Keras](https://keras.io/). This semester, we've been able to focus completely on model development and assessment and have created 2 unsuccessful models and 1 moderately successful model. \n",
    "\n",
    "Our initial model was of the Xception architecture **(XCEPTION CITATION)** and was trained on the untouched dataset divided into subclasses. Xception is an adjusted version of Google's publicly released Inception model **(INCEPTION CITATION)**. Both architectures contain specialized layers where multiple convolutional operations are performed on the same tensor. Inception modules concatonate the outputs of these operations while Xception maps the all outputs directly to a tensor of the same x and y dimensions. \n",
    "\n",
    "\n",
    "<img src=\"inception_module.JPG\" alt=\"inception_module\" style=\"width: 400px;\"/>\n",
    "<img src=\"xception_module.JPEG\" alt=\"xception_module\" style=\"width: 400px;\"/>\n",
    "\n",
    "We chose Xception because it's been reliably proven to be more accurate than the other architecture while also being relatively quick to train ([compared to the other architectures available natively within keras](https://keras.io/applications/)). This model proved to be a failure due to reasons discussed in the **Dataset** section. This model was extremely biased towards classifying images as being heathy. \n",
    "\n",
    "As the use of single stage algorithms wasn’t proving successful, we decided to try a two stage object detection network called RetinaNet. The backbone of this model is an algorithm known as a feature pyramid network (FPN). The feature pyramid takes an input image and breaks it down to a range of different resolutions, forming a feature map out of them (Lin et al. 2017). The feature map at each resolution, or layer, is then fed in succession through two fully convolutional neural networks (FCN’s). The first stage of object detection, carried out by the first FCN,  is object classification. The algorithm breaks the image from the given FPN level into candidate regions, enclosing each one with an anchor box. These candidate regions are parts of the image that the network, based on its training, suspect contain pneumonia. This new image, which contains the anchor boxes, is then fed into the second FCN for the second stage: box regression. This FCN takes a closer, more precise look at the enclosed regions and attempts to resize the anchor boxes to closely fit determined cases of pneumonia. If it is determined that there is no pneumonia within the anchor box, the box will be deleted entirely. The output of each layer is then compared, and the information is accumulated to produce one output image. Ideally, this image will be blank if healthy, or have anchor boxes tightly fit around any cases of pneumonia (Lin et al. 2017).\n",
    "\n",
    "Our hope in this new approach was that we could improve training speed and classification accuracy by avoiding class imbalance. Class imbalance in this case describes a situation where the classes the network is trying to identify are scarce within each training image, and there is a lot of empty space void of useful information (Lin et al. 2017). We found in our previous attempts that using one network to closely analyze entire images of lungs wasted a lot of training time, as many pneumonia cases exist only in scarce regions. Close analysis of the vast, healthy parts of the lungs is not necessary. We believed that RetinaNet would solve this problem, as it throws away most of the unnecessary information in the first stage. \n",
    "\n",
    "As we are fairly new to programming neural networks, and RetinaNet is an existing model that has been successful on other datasets, we decided to utilize a premade model and alter the subnetworks. Here is the repository from which we got the initial code: https://github.com/fizyr/keras-retinanet. We changed the image classification subnetwork to a preset model called ResNet50, which we hoped would better suit our type of training data. Ultimately, the model was not successful in identifying pneumonia, returning an accuracy below 0.01. On close analysis of our model, we found that the issue was is the classification stage. When evaluated on testing data, anchor boxes were never produced at all, meaning that the classification process was not detailed enough to identify any candidate regions, and the regression network never got an input. \n",
    "\n",
    "\n",
    "Unfortunately, our team lost the hard drive containing training and validation metrics for our first two models so will not be able to discuss these models' results in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Semester's Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intro:**\n",
    "\n",
    "This semester\n",
    "- Dataset **(BURACK)** https://www.kaggle.com/c/rsna-pneumonia-detection-challenge?login=true\n",
    "    - Discuss dimensionality issues\n",
    "    - Discuss discrepencies in classes\n",
    "- Preprocessing of data **(JACK)**\n",
    "    - Discuss techniques such as over/under-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**:\n",
    "- Tensorboard graphs (BOTH MODELS) **(JACK)**\n",
    "    - Interpretations\n",
    "- Evaluate network on test imaging **(JACK)**\n",
    "    - Discuss how discrepencies in dataset could account for results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xception training accuracy\n",
    "![alt_text](training_accuracy_xception_v2.PNG)\n",
    "#### Xception training loss\n",
    "![alt_text](training_loss_xception_v2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Improvements for Next Semester: **(JACK)**\n",
    "\n",
    "While this semester's results aren't ideal, we've built a robust codebase to build upon. In less than a year, our group has gone from not knowing anything about neural networks to developing and evaluating multiple models rapidly. \n",
    "\n",
    "- Ensembling models\n",
    "- Evaluate new methods from literature\n",
    "- Evaluate current infrastructure on new dataset\n",
    "- Train more models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "In conclusion, our"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will evaluate the use of convolutional neural networks (CNNs) in classyfing disease from medical imaging. Last semester, we performed the research necessary to sufficiently understand neural networks. This semester's goal is to create a functional model **CHANGE THIS with accuracy equal to that of human doctors CHANGE THIS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Snippits to Write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate trained Network on Test Data**\n",
    "- Load model\n",
    "- Run model on test data; save history\n",
    "- Display metrics in matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Draw box around pneumonia**\n",
    "- Rework old matplotlib solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble method with multiple networks**:\n",
    "- Figure out keras API to manipulate multiple networks\n",
    "- Take outputs, average them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import keras\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras.layers as layers\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Average, Input\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xception_final = load_model('xception15.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 208 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "data_dir = r'C:\\Users\\jecks\\Documents\\School\\GCI\\gci_data\\test'\n",
    "\n",
    "test_datagen = ImageDataGenerator()\n",
    "test_datagen = test_datagen.flow_from_directory(data_dir,\n",
    "                                                batch_size = 1,\n",
    "                                                target_size=(1024,1024),\n",
    "                                                class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/208 [=================>............] - ETA: 16:37"
     ]
    }
   ],
   "source": [
    "test_history = xception_final.evaluate_generator(test_datagen, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(models, model_input):\n",
    "    \n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    y = Average()(outputs)\n",
    "    \n",
    "    model = Model(model_input, y, name='ensemble')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembleModels(models, model_input):\n",
    "    # collect outputs of models in a list\n",
    "    yModels=[model(model_input) for model in models] \n",
    "    # averaging outputs\n",
    "    yAvg=layers.average(yModels) \n",
    "    # build model from same input and avg output\n",
    "    modelEns = Model(inputs=model_input, outputs=yAvg,    name='ensemble')  \n",
    "   \n",
    "    return modelEns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_input = Input(shape=models[0].input_shape[1:]) \n",
    "modelEns = ensembleModels(models, model_input)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theWholeSquad = ensemble(models, models[0].input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- https://www.ruralhealthinfo.org/topics/healthcare-access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chollet F. 2017. Xception: Deep Learning with Depthwise Separable Convolutions. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
